<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Identifying Fraud from Enron Email</title>

    <!-- Bootstrap Core CSS -->
    <link href="/css/bootstrap.min.css" rel="stylesheet">
	<link href="/css/blog.css" rel="stylesheet">
    <!-- Custom CSS -->
	<link href="/css/fifa.css" rel="stylesheet">
    <!-- jQuery -->
    <script src="/js/jquery.js"></script>
	<script src="/js/d3.min.js"></script>
	<script src="/js/queue.v1.min.js"></script>
	<script src="/js/topojson.v1.min.js"></script>  
	<script src="/js/d3.tip.v0.6.3.js"></script>	
    <script src="/js/d3.fifa.js"></script>
	<!-- Bootstrap Core JavaScript -->
    <script src="/js/bootstrap.min.js"></script> 
	
    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
<script>
$(document).ready(function(e) {
//	
SvgModule.init();
$(".tab").on("click", SvgModule.selectGraph);
$("#sort_by").on("change", SvgModule.selectGraph);
$("#play_button").on("click", SvgModule.playButtonHandler);
});
</script>

</head>
<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="/">Home</a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                
				<ul class="nav navbar-nav">
                    <li>
                        <a href="/portfolio.html">My projects</a>
                    </li>
				<!--
                    <li>
                        <a href="/links.html">Useful links</a>
                    </li>

					<li>
                        <a href="#">Interesting gists</a>
                    </li>
				-->
				</ul>

            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>
    <!-- Page Content -->
    <div class="container">



    
    
        
            
        
            
        
            
                
            
        
        
            
            
        
    



		<h2>Identifying Fraud from Enron Email</h2>
		<p class="post_second_level_header">
			<span class="glyphicon glyphicon-calendar"></span>28 Feb 2015
			<span class="glyphicon glyphicon-comment"></span><a href="https://alexbra.github.io/2015/02/28/projects.ml/#disqus_thread" data-disqus-identifier="/2015/02/28/projects.ml/"></a>
			<span class="glyphicon glyphicon-folder-open"></span>nanodegree
			<span class="glyphicon glyphicon-tags"></span><i class="fa fa-tag"></i>: <a href="/blog/tag/machine-learning/"><span class="label label-info">machine-learning</span></a>
		</p>
		<hr>
		<div id="post_content" class="post-content">
		<p>In this project I’ve put my machine learning skills to use by building an algorithm to identify Enron Employees 
who may have committed fraud based on the public Enron financial and email dataset.</p>

<h4 id="code-source-on-github">Code source on GitHub</h4>
<p><a href="https://github.com/alexbra/ud-nd-project4">https://github.com/alexbra/ud-nd-project4</a></p>

<h2 id="dataset-and-goal-of-project">1. Dataset and goal of project</h2>

<h4 id="goal">Goal</h4>

<p>The main purpose of project is develop the machine learning algorithm to detect person of interest(POI) from dataset.
A POI is someone who was indicted for fraud, settled with the government, or testified in exchange for immunity.</p>

<h4 id="dataset">Dataset</h4>
<p>We have Enron email+financial (E+F) dataset. It contains 146 Enron managers to investigate. Each sample in this dictionary containing 21 features. 
18 people from this dataset labeled as POI. All of them have <code>poi</code> feature set as <strong>True</strong>. There’s two imbalanced classes (many more non-POIs than POIs).</p>

<p>There’s example of one POI data point:</p>

<pre><code>[{"SKILLING JEFFREY K":{
			'salary': 1111258, 
			'to_messages': 3627, 
			'deferral_payments': 'NaN', 
			'total_payments': 8682716, 
			'exercised_stock_options': 19250000, 
			'bonus': 5600000, 
			'restricted_stock': 6843672, 
			'shared_receipt_with_poi': 2042, 
			'restricted_stock_deferred': 'NaN', 
			'total_stock_value': 26093672, 
			'expenses': 29336, 
			'loan_advances': 'NaN', 
			'from_messages': 108, 
			'other': 22122, 
			'from_this_person_to_poi': 30, 
			'poi': True, 
			'director_fees': 'NaN', 
			'deferred_income': 'NaN', 
			'long_term_incentive': 1920000, 
			'email_address': 'jeff.skilling@enron.com', 
			'from_poi_to_this_person': 88
			}
}]
</code></pre>

<h4 id="outliers">Outliers</h4>
<p>Dataset contains some outliers. The TOTAL row is the biggest Enron E+F dataset outlier. We should remove it from dataset for reason it’s a spreadsheet quirk.
Moreover, there’s 4 more outliers with big salary and bonus. Two people made bonuses more than 6 million dollars, and a salary of over 1 million dollars. 
There’s no mistake. Ken Lay and Jeffrey Skilling made such money. So, leave these data points in and examine it with others.</p>

<h2 id="feature-selection-process">2. Feature selection process</h2>

<h4 id="i-selected-the-following-features">I selected the following features</h4>
<p><code>exercised_stock_options</code>  <code>shared_receipt_with_poi</code>  <code>fraction_from_poi</code>  <code>expenses</code>  <code>other</code> <code>salary</code></p>

<h4 id="new-features">New features</h4>
<p>In addition I create two new features which were considered in course:
* <code>fraction_from_poi</code> fraction of messages to that person from a POI
* <code>fraction_to_poi</code> fraction of messages from that person to a POI
They created on assumption POI have more intensive correspondence to each other.</p>

<p>Also I create another feature. Messages to current person from specific email addresses, which belong to four POI outliers (e.g. Ken Lay etc.)
* <code>from_specific_email</code></p>

<h4 id="plots-of-the-new-features">Plots of the new features</h4>
<p><img src="https://github.com/alexbra/ud-nd-project4/blob/master/img/figure_1.png" alt="Figure 1" />
<img src="https://github.com/alexbra/ud-nd-project4/blob/master/img/figure_2.png" alt="Figure 2" /></p>

<p>Feature selection process include several iterations. 
On the first step I created set of features based on data visualization and intuition. Then I examine three classificator on this features. Dtecision Trees was selected as main algorithm. 
Since I choose Decision Trees as a classificator, I used feature importance method to optimize features for this dataset.</p>

<p>As a result I’ve received the following feature importances:</p>

<pre><code>Rank of features
0.224388 : other
0.217197 : exercised_stock_options
0.195282 : shared_receipt_with_poi
0.185831 : expenses
0.145820 : fraction_from_poi
0.031483 : salary
</code></pre>

<p>fraction_from_poi looks like important feature and I leave it in. fraction_to_poi has importance equal to zero, so I’ve took in out from algorithm. Pearson who received e-mails from POI more often than others looks like POI himself. I also tried to change some features by hand but each time received worse performance.</p>

<p>To see full log of feature selection process go to the next block of this page.</p>

<h2 id="pick-an-algorithm">3. Pick an algorithm</h2>
<p>I tried the Naive Bayes, SVM and Decision Trees algorithms.</p>

<h4 id="all-results-of-examination-i-included-in-the-following-table">All results of examination I included in the following table</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Algorithm</th>
      <th>Accuracy</th>
      <th>Precisions</th>
      <th>Recall</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Naive Bayes</strong></td>
      <td>0.82100</td>
      <td>0.29026</td>
      <td>0.23700</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Decision Trees</strong></td>
      <td>0.82620</td>
      <td>0.34617</td>
      <td>0.34150</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>SVM</strong></td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p>SVM algorithm returned the next error :
<code>
Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
</code></p>

<h4 id="chosen-algorithm">Chosen algorithm</h4>
<p>Based on best performance level I picked Decision Trees as a final algorithm.</p>

<h2 id="tune-the-algorithm">4. Tune the algorithm</h2>

<h4 id="reasons-for-algorithm-tuning">Reasons for algorithm tuning</h4>
<p>The main reason is get better results from algorithm. Parameters of ML classifiers have a big influence in output results. 
The purpose of tuning is to find best sets of parameters for particular dataset.</p>

<h4 id="gridsearchcv">GridSearchCV</h4>
<p>I apply GridSearchCV to tune the following parameters</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Parameter</th>
      <th style="text-align: left">Settings for investigation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">min_samples_split</td>
      <td style="text-align: left">[2,6,8,10]</td>
    </tr>
    <tr>
      <td style="text-align: left">Splitter</td>
      <td style="text-align: left">(random,best)</td>
    </tr>
    <tr>
      <td style="text-align: left">max_depth</td>
      <td style="text-align: left">[None,2,4,6,8,10,15,20]</td>
    </tr>
  </tbody>
</table>

<p>As a result, I received better performance with <code>min_samples_split</code> = ‘12’ and <code>Splitter</code> = ‘best’ <code>and max_depth</code> = ‘6’</p>

<h2 id="validation">5. Validation</h2>
<p>According to <a href="http://scikit-learn.org/stable/modules/cross_validation.html">sklearn documentation</a> one of the main and classical mistakes in validation is using the same data for both training and testing. 
&gt;Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: 
&gt;a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting.</p>

<p>To validate my analysis I used <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html">stratified shuffle split cross validation</a> developed by Udacity and defined in tester.py file</p>

<h2 id="evaluation-metrics">6. Evaluation metrics</h2>
<p>I used precision and recall evaluation metrics to estimate model.
Final results can be found in table below</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Metric</th>
      <th style="text-align: left">Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Precision</strong></td>
      <td style="text-align: left"><strong>0.52166</strong></td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Recall</strong></td>
      <td style="text-align: left"><strong>0.41550</strong></td>
    </tr>
    <tr>
      <td style="text-align: left">Accuracy</td>
      <td style="text-align: left">0.86207</td>
    </tr>
    <tr>
      <td style="text-align: left">True positives</td>
      <td style="text-align: left">831</td>
    </tr>
    <tr>
      <td style="text-align: left">False positives</td>
      <td style="text-align: left">762</td>
    </tr>
    <tr>
      <td style="text-align: left">False negatives</td>
      <td style="text-align: left">1169</td>
    </tr>
    <tr>
      <td style="text-align: left">True negatives</td>
      <td style="text-align: left">11238</td>
    </tr>
  </tbody>
</table>

<h4 id="conclusion">Conclusion</h4>
<p>Precision and Recall have almost identical values and both higher than .3. Thus, project goal was reached.
Precision 0.52166 means when model detect person as POI it was true only in 52% cases. 
At the same time Recall = 0.41550 says only 41% of all POIs was detected.</p>

<p>We have very imbalanced classes in E+F dataset. In addition, almost half of all POIs weren’t included in dataset. 
In such conditions result we received good enough, but it’s not perfect, of course.</p>

<h1 id="algorithm-outputs-log">Algorithm outputs log</h1>

<h4 id="step1-init-stage-select-classificator">STEP1. Init stage. Select classificator</h4>

<h5 id="featureslist">features_list:</h5>
<pre><code>features_list = ['poi',
             'fraction_to_poi',
             'fraction_from_poi',
             'from_specific_email',
             'from_messages',
             'exercised_stock_options',
             'shared_receipt_with_poi',
             'expenses',
             'other',
             'bonus',
             'salary',
             'total_stock_value'] 
</code></pre>

<h5 id="classificator">Classificator:</h5>
<pre><code>clf = tree.DecisionTreeClassifier()
</code></pre>

<h5 id="metrics">Metrics:</h5>
<pre><code>Accuracy: 0.82620	Precision: 0.34617	Recall: 0.34150	F1: 0.34382	F2: 0.34242
Total predictions: 15000	
True positives:  683	False positives: 1290	
False negatives: 1317	True negatives: 11710
</code></pre>

<h5 id="classificator-1">Classificator:</h5>
<pre><code>clf = GaussianNB()
</code></pre>

<h5 id="metrics-1">Metrics:</h5>
<pre><code>Accuracy: 0.82100	Precision: 0.29026	Recall: 0.23700	F1: 0.26094	F2: 0.24603
Total predictions: 15000	
True positives:  474	False positives: 1159	
False negatives: 1526	True negatives: 11841
</code></pre>

<h4 id="step2-select-features-by-decision-trees-featureimportances">STEP2. Select features by Decision Trees feature_importances_</h4>

<h5 id="featureimportances">feature_importances_</h5>
<pre><code>Rank of features
0.260361 : other
0.231356 : exercised_stock_options
0.225797 : expenses
0.134677 : fraction_from_poi
0.118998 : shared_receipt_with_poi
0.028810 : salary
0.000000 : fraction_to_poi
0.000000 : from_specific_email
0.000000 : from_messages
0.000000 : bonus
0.000000 : total_stock_value
</code></pre>

<h5 id="metrics-after-optimizing">Metrics after optimizing</h5>
<pre><code>Accuracy: 0.84029	Precision: 0.43873	Recall: 0.42250	F1: 0.43046	F2: 0.42565
Total predictions: 14000	
True positives:  845	False positives: 1081	
False negatives: 1155	True negatives: 10919
</code></pre>

<h4 id="step3-tune-the-algorithm">STEP3. Tune the algorithm</h4>

<h5 id="featureslist-1">features_list</h5>
<pre><code>features_list = ['poi',
             'salary',
             'fraction_from_poi',
             'exercised_stock_options',
             'shared_receipt_with_poi',
             'expenses',
             'other'] 
</code></pre>

<h5 id="best-estimator">best estimator:</h5>
<pre><code>DecisionTreeClassifier(compute_importances=None, criterion='gini',
        max_depth=6, max_features=None, max_leaf_nodes=None,
        min_density=None, min_samples_leaf=1, min_samples_split=12,
        random_state=None, splitter='best')
</code></pre>

<h5 id="metrics-after-tuning-best-choise">Metrics after tuning (BEST CHOISE!)</h5>
<pre><code>Accuracy: 0.86207	Precision: 0.52166 Recall: 0.41550 F1: 0.46257	F2: 0.43313
Total predictions: 14000	
True positives:  831	False positives:  762	
False negatives: 1169	True negatives: 11238
</code></pre>

<h4 id="step4-change-features-by-hand-examine-only-email-features">STEP4. Change features by hand (examine only email features)</h4>
<pre><code>features_list = ['poi',
             'fraction_from_poi',
             'fraction_to_poi',                 
             'exercised_stock_options',
             'shared_receipt_with_poi'] 
</code></pre>

<h5 id="metrics-2">Metrics</h5>
<pre><code>Accuracy: 0.83108	Precision: 0.43510	Recall: 0.32850	F1: 0.37436	F2: 0.34543
Total predictions: 13000	
True positives:  657	False positives:  853	
False negatives: 1343	True negatives: 10147
</code></pre>

<h4 id="step5-tune-parameters-by-hand">STEP5. Tune parameters by hand</h4>

<h5 id="parameters">parameters</h5>
<pre><code>clf = tree.DecisionTreeClassifier(random_state=42, min_samples_split=2,max_depth=2, splitter='best')
</code></pre>

<h5 id="metrics-3">Metrics</h5>
<pre><code>Accuracy: 0.83550	Precision: 0.37947	Recall: 0.23850	F1: 0.29291	F2: 0.25764
Total predictions: 14000	
True positives:  477	False positives:  780	
False negatives: 1523	True negatives: 11220
</code></pre>

<h4 id="step6-final-choise">STEP6. Final choise</h4>

<h5 id="featureslist-2">features_list</h5>
<pre><code>features_list = ['poi',
             'fraction_from_poi',
             'fraction_to_poi',                 
             'exercised_stock_options',
             'shared_receipt_with_poi'] 
</code></pre>

<h5 id="parameters-1">parameters</h5>
<pre><code>clf = tree.DecisionTreeClassifier(random_state=42, min_samples_split=12,max_depth=6, splitter='best'	
</code></pre>

<h5 id="metrics-4">Metrics</h5>
<pre><code>Accuracy: 0.86207	Precision: 0.52166	Recall: 0.41550	F1: 0.46257	F2: 0.43313
Total predictions: 14000	
True positives:  831	False positives:  762	
False negatives: 1169	True negatives: 11238
</code></pre>

<h1 id="related-links">Related links</h1>
<ul>
  <li><a href="http://scikit-learn.org/stable/documentation.html">Documentation of scikit-learn 0.15</a></li>
  <li><a href="http://amueller.github.io/sklearn_tutorial/">sklearn tutorial</a></li>
  <li><a href="http://topepo.github.io/caret/rfe.html">Recursive Feature Elimination</a></li>
  <li><a href="http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/">Selecting good features – Part I: univariate selection</a></li>
  <li><a href="http://bazhenov.me/blog/2012/07/21/classification-performance-evaluation.html">Cross-validation: the right and the wrong way</a></li>
  <li><a href="http://bazhenov.me/blog/2012/07/21/classification-performance-evaluation.html">Accuracy, Precision and Recall(in Russian)</a></li>
</ul>


		</div>
<hr>
<!-- Add Disqus comments. -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * ADD YOUR DISQUS SHORTNAME HERE * * */
  var disqus_shortname = 'alexbrablog';
  
  (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by  <span class="logo-disqus">Disqus</span></a>



        <hr>
	
        <!-- Footer -->
        <footer>
			
            <div class="row">
                <div class="col-lg-12">
						<a href="https://www.linkedin.com/profile/view?id=109443060"><span class="span_in"></span></a>
						<a href="https://github.com/alexbra"><span class="span_git"></span></a>
						<a href="skype:alex_bra_don?add"><span class="span_skype"></span></a>				
                </div>
                <!-- /.col-lg-12 -->
            </div>
            <!-- /.row -->
        </footer>
    </div>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'alexbrablog'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = '//' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script> 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-7282037-16', 'auto');
  ga('send', 'pageview');

</script>
</body>

